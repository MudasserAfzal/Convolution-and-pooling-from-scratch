{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import scipy.io\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(path):\n",
    "    print('Loading Dataset...')\n",
    "    train_x, train_y, test_x, test_y = [], [], [], []\n",
    "    for i in range(10):\n",
    "        for filename in glob.glob(path + '\\\\train\\\\' + str(i)+'\\\\*.png'):\n",
    "            im=cv2.imread(filename)\n",
    "            im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "            train_x.append(im)\n",
    "            train_y.append(i)\n",
    "    for i in range(10):\n",
    "        for filename in glob.glob(path + '\\\\test\\\\' + str(i)+'\\\\*.png'):\n",
    "            im=cv2.imread(filename)\n",
    "            im = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "            test_x.append(im)\n",
    "            test_y.append(i)\n",
    "    print('Dataset loaded...')\n",
    "    return np.array(train_x), np.array(train_y), np.array(test_x),np.array(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n",
      "Dataset loaded...\n"
     ]
    }
   ],
   "source": [
    "train_set_x1, train_set_y1, test_set_x1, test_set_y1  = loadDataset(r'C:\\Users\\Mudasser Afzal\\Desktop\\deep ass 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y, test_set_x, test_set_y = train_set_x1, train_set_y1, test_set_x1, test_set_y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (60000, 28, 28)\n",
      "train_y shape: (60000,)\n",
      "test_x shape: (10000, 28, 28)\n",
      "test_y shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x shape:\", train_set_x.shape)\n",
    "print(\"train_y shape:\", train_set_y.shape)\n",
    "print(\"test_x shape:\", test_set_x.shape)\n",
    "print(\"test_y shape:\", test_set_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x, train_set_y = shuffle(train_set_x, train_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reshape and Flattening Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_y = np.expand_dims(train_set_y, axis = 1)\n",
    "test_set_y = np.expand_dims(test_set_y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (60000, 28, 28)\n",
      "train_y shape: (60000, 1)\n",
      "test_x shape: (10000, 28, 28)\n",
      "test_y shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x shape:\", train_set_x.shape)\n",
    "print(\"train_y shape:\", train_set_y.shape)\n",
    "print(\"test_x shape:\", test_set_x.shape)\n",
    "print(\"test_y shape:\", test_set_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean subtraction and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = (train_set_x - int(np.mean(train_set_x))) / 255\n",
    "test_set_x = (test_set_x - int(np.mean(test_set_x))) / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(Y):\n",
    "    c = np.arange(10)\n",
    "    encoding = (c == Y).astype(np.int)\n",
    "    return encoding\n",
    "\n",
    "train_set_y = encoder(train_set_y)\n",
    "test_set_y = encoder(test_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (60000, 28, 28)\n",
      "train_y shape: (60000, 10)\n",
      "test_x shape: (10000, 28, 28)\n",
      "test_y shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x shape:\", train_set_x.shape)\n",
    "print(\"train_y shape:\", train_set_y.shape)\n",
    "print(\"test_x shape:\", test_set_x.shape)\n",
    "print(\"test_y shape:\", test_set_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 17, 36)\n"
     ]
    }
   ],
   "source": [
    "data = scipy.io.loadmat('filters.mat')\n",
    "filters = data['filters']\n",
    "filters = np.array(filters)\n",
    "print(filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth = filters.shape[2]\n",
    "# for i in range(depth):\n",
    "#     plt.subplot(5,8,i+1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(filters[:,:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution and Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(image, filter):\n",
    "    s = np.multiply(image, filter)\n",
    "    Z = np.sum(s)\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward(img, filters, stride=1, pad=0):\n",
    "#     print(img.shape)\n",
    "    output_size_H = int(((img.shape[1] - len(filters) + (2 * pad)) / (stride)) + 1)\n",
    "    output_size_W = int(((img.shape[0] - len(filters) + (2 * pad)) / (stride)) + 1)\n",
    "#     print(\"the shape of output will be :\", output_size_H, output_size_W)\n",
    "\n",
    "    Z = np.zeros([output_size_H, output_size_W, filters.shape[2]])\n",
    "    for d in range(filters.shape[2]):\n",
    "        a = filters[:,:,d]\n",
    "        for j in range(0, output_size_H):\n",
    "            for k in range(0, output_size_W):\n",
    "                img_patch = img[(j*stride):(j*stride)+(filters.shape[1]), (k*stride):(k*stride)+(filters.shape[0])]\n",
    "                Z[j, k, d] = conv_single_step(img_patch, a)\n",
    "\n",
    "    return Z\n",
    "\n",
    "def pool_forward(feature_map, mode = \"max\", filter_size=2, stride=2):\n",
    "    \n",
    "    pool_size_H = int(((feature_map.shape[0] - filter_size) / (stride)) + 1)\n",
    "    pool_size_W = int(((feature_map.shape[1] - filter_size) / (stride)) + 1)\n",
    "#     print(\"the shape of output after pooling will be :\", pool_size_H,\"x\", pool_size_W)\n",
    "    \n",
    "    A = np.zeros([pool_size_H, pool_size_W, feature_map.shape[2]])\n",
    "    for d in range(feature_map.shape[2]):\n",
    "        for j in range(0, pool_size_H):\n",
    "            for k in range(0, pool_size_W):\n",
    "                img_patch = feature_map[(j*stride):(j*stride)+(filter_size), (k*stride):(k*stride)+(filter_size)]\n",
    "                if mode == \"max\":        \n",
    "                    A[j, k, d] = np.max(img_patch)\n",
    "                elif mode == \"average\":\n",
    "                    A[j, k, d] = np.mean(img_patch)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training = []\n",
    "testing = []\n",
    "for img in train_set_x:\n",
    "    Z = conv_forward(img, filters, stride=1, pad=0)\n",
    "    A = pool_forward(Z, mode = \"max\", filter_size=2, stride=2)\n",
    "    training.append(A)\n",
    "    \n",
    "train_flatten = np.asarray(training)\n",
    "train_flatten = train_flatten.flatten().reshape(train_flatten.shape[0],(train_flatten.shape[1] * train_flatten.shape[2]* train_flatten.shape[3]))\n",
    "\n",
    "\n",
    "for img in test_set_x:\n",
    "    Z1 = conv_forward(img, filters, stride=1, pad=0)\n",
    "    A1 = pool_forward(Z1, mode = \"max\", filter_size=2, stride=2)\n",
    "    testing.append(A1)\n",
    "    \n",
    "test_flatten = np.asarray(testing)\n",
    "test_flatten = test_flatten.flatten().reshape(test_flatten.shape[0],(test_flatten.shape[1] * test_flatten.shape[2]* test_flatten.shape[3]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth = A.shape[2]\n",
    "# for i in range(depth):\n",
    "#     plt.subplot(5,8,i+1)\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(A[:,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1296)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_flatten.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = train_flatten[:50000]\n",
    "trainY = train_set_y[:50000]\n",
    "validX = train_flatten[50000:]\n",
    "validY = train_set_y[50000:]\n",
    "testX = test_flatten\n",
    "testY = test_set_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (50000, 1296)\n",
      "training label shape: (50000, 10)\n",
      "validation data shape: (10000, 1296)\n",
      "validation label shape: (10000, 10)\n",
      "testing data shape: (10000, 1296)\n",
      "testing label shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"training data shape:\", trainX.shape)\n",
    "print(\"training label shape:\", trainY.shape)\n",
    "print(\"validation data shape:\", validX.shape)\n",
    "print(\"validation label shape:\", validY.shape)\n",
    "print(\"testing data shape:\", testX.shape)\n",
    "print(\"testing label shape:\", testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    \n",
    "    def __init__(self, neurons_per_layer, no_of_layers = 2, inputSize = 1296):        \n",
    "        # size of layers\n",
    "        self.inputSize = inputSize\n",
    "        self.hidden1 = neurons_per_layer[0]\n",
    "        self.hidden2 = neurons_per_layer[1]\n",
    "        self.outputSize = neurons_per_layer[2] \n",
    "        #weights\n",
    "        self.model = {}\n",
    "        w1 = np.random.randn(self.inputSize, self.hidden1) #1296x128\n",
    "        b1 = np.ones((1,self.hidden1))\n",
    "        w2 = np.random.randn(self.hidden1, self.hidden2)   #128x64\n",
    "        b2 = np.ones((1,self.hidden2))\n",
    "        w3 = np.random.randn(self.hidden2, self.outputSize) #64x10\n",
    "        b3 = np.ones((1,self.outputSize))\n",
    "\n",
    "        self.model = {'w1': w1 , 'b1': b1, 'w2': w2 , 'b2': b2, 'w3': w3 , 'b3': b3}  \n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        activated_output = 1 / (1 + (np.exp(-s)))\n",
    "        return activated_output\n",
    "\n",
    "    def sigmoid_derivative(self, s):\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def softmax(self, s):\n",
    "        e = np.exp(s - np.max(s))\n",
    "        return e/ np.sum(e, axis=1, keepdims=True)\n",
    "\n",
    "    def softmax_derivative(self, s):\n",
    "        z = self.softmax(s)\n",
    "        D = -np.outer(z, z) + np.diag(z.flatten())\n",
    "        return D\n",
    "    \n",
    "    def feedforward(self, X):\n",
    "        forward_result = {}\n",
    "        \n",
    "        w1, b1, w2, b2, w3, b3 = self.model['w1'], self.model['b1'], self.model['w2'], self.model['b2'], self.model['w3'], self.model['b3']\n",
    "        \n",
    "        z1 = X.dot(w1) + b1\n",
    "        a1 = self.sigmoid(z1)\n",
    "       \n",
    "        z2 = a1.dot(w2) + b2\n",
    "        a2 = self.sigmoid(z2)\n",
    "    \n",
    "        z3 = a2.dot(w3) + b3\n",
    "        a3 = self.softmax(z3)\n",
    "\n",
    "        forward_result = {'z1': z1 , 'a1': a1, 'z2': z2 , 'a2': a2, 'z3': z3 , 'a3': a3}\n",
    "        return forward_result\n",
    "    \n",
    "    def back_propagation(self, X, Y, forward_result, lr):\n",
    "        \n",
    "        w1, b1, w2, b2, w3, b3 = self.model['w1'],self.model['b1'], self.model['w2'],self.model['b2'], \\\n",
    "        self.model['w3'],self.model['b3']\n",
    "        \n",
    "        gradient = {}\n",
    "    \n",
    "        a1, a2, a3 = forward_result['a1'], forward_result['a2'], forward_result['a3']\n",
    "    \n",
    "        m = X.shape[0]\n",
    "        \n",
    "        delta_out = a3 - Y\n",
    "        delta_w3 = 1/m * np.dot(a2.T, delta_out)\n",
    "        delta_b3 = 1/m * np.sum(delta_out, axis=0, keepdims = True)\n",
    "        \n",
    "        delta_h2 = np.multiply(np.dot(delta_out, w3.T) , self.sigmoid_derivative(a2))\n",
    "        delta_w2 = 1/m * np.dot(a1.T, delta_h2)\n",
    "        delta_b2 = 1/m * np.sum(delta_h2, axis=0, keepdims = True)\n",
    "        \n",
    "\n",
    "        delta_h1 = np.multiply(np.dot(delta_h2, w2.T) , self.sigmoid_derivative(a1))\n",
    "        delta_w1 = 1/m * np.dot(X.T, delta_h1)\n",
    "        delta_b1 = 1/m * np.sum(delta_h1, axis=0, keepdims = True)\n",
    "        \n",
    "        gradient = {'dw3':delta_w3, 'db3':delta_b3, 'dw2':delta_w2, 'db2':delta_b2, 'dw1':delta_w1,'db1':delta_b1}\n",
    "        return gradient\n",
    "    \n",
    "       \n",
    "    def weight_update(self, model, gradient, lr):\n",
    "        \n",
    " ###################### weight Updating here #####################\n",
    "        \n",
    "        self.model['w1'] = self.model['w1'] - (lr * gradient['dw1'])\n",
    "        self.model['b1'] = self.model['b1'] - (lr * gradient['db1'])\n",
    "        self.model['w2'] = self.model['w2'] - (lr * gradient['dw2'])\n",
    "        self.model['b2'] = self.model['b2'] - (lr * gradient['db2'])\n",
    "        self.model['w3'] = self.model['w3'] - (lr * gradient['dw3'])\n",
    "        self.model['b3'] = self.model['b3'] - (lr * gradient['db3'])\n",
    "            \n",
    "    def crossentropy(self, Y, Y_pred):\n",
    "        return (-np.sum(Y * np.log2(Y_pred+1e-12))) / len(Y)\n",
    "    \n",
    "    def batch_gradient(self, batch_size, trainX, trainY, learningRate):\n",
    "        for j in range(0, trainX.shape[0], batch_size):\n",
    "            X = trainX[j:j+batch_size]\n",
    "            Y = trainY[j:j+batch_size]\n",
    "            forward_result = self.feedforward(X)\n",
    "            gradient = self.back_propagation(X, Y, forward_result, learningRate)\n",
    "            self.weight_update(self.model, gradient, learningRate)\n",
    "\n",
    "    def train(self, trainX, trainY, batch_size, epochs, learningRate,validationX = 'None', validationY = 'None'):\n",
    "\n",
    "        train_error = []\n",
    "        valid_error = []\n",
    "        valid_accu = []\n",
    "        train_accu = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            \n",
    "            self.batch_gradient(batch_size, trainX, trainY, learningRate)\n",
    "            \n",
    "            train_pred = self.feedforward(trainX)\n",
    "            entropy = self.crossentropy(trainY, train_pred['a3'])\n",
    "            train_error.append(entropy)\n",
    "            \n",
    "            taccu = self.accuracy(trainX, trainY)\n",
    "            train_accu.append(taccu)\n",
    "            \n",
    "            val_pred = self.feedforward(validationX)\n",
    "            err = self.crossentropy(validationY, val_pred['a3'])\n",
    "            valid_error.append(err)                \n",
    "\n",
    "            accu= self.accuracy(validationX, validationY)\n",
    "            valid_accu.append(accu)\n",
    "            \n",
    "            print (\"epoch:\",i, \"training Loss:\" ,entropy, \"validation Loss:\" ,err, \\\n",
    "                      \"train accuracy\", taccu, \"valid accuracy\", accu)\n",
    "            \n",
    "        self.curve_plot(epochs, train_error, valid_error, train_accu, valid_accu)\n",
    "        \n",
    "    def curve_plot(self, epochs, train_error, valid_error, train_accu, valid_accu):\n",
    "        \n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"loss curve\")\n",
    "        plt.plot(range(epochs), train_error)\n",
    "        plt.plot(range(epochs), valid_error)\n",
    "        plt.show()\n",
    "        plt.legend(loc='best')\n",
    "        plt.title(\"accuracy Curve\")\n",
    "        plt.plot(range(epochs), train_accu)\n",
    "        plt.plot(range(epochs), valid_accu)\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, X):\n",
    "        forward_result = self.feedforward(X)\n",
    "        return forward_result['a3']\n",
    "    \n",
    "    def accuracy(self, X, Y):\n",
    "    \n",
    "        test_pred = self.predict(X)\n",
    "        pred = np.argmax(test_pred, axis=1)\n",
    "        y = np.argmax(Y, axis=1)        \n",
    "        accuracy = (pred == y)\n",
    "        accuracy = np.mean(accuracy)\n",
    "        return accuracy * 100\n",
    "        \n",
    "    def saveModel(self, name):\n",
    "        np.savez(name, self.model['w1'],self.model['b1'],self.model['w2'],\\\n",
    "                 self.model['b2'],self.model['w3'],self.model['b3'], allow_pickle=True)\n",
    "        \n",
    "    def loadModel(self,name):\n",
    "        file = np.load(name, allow_pickle=True)\n",
    "        self.model['w1'] = file['arr_0']\n",
    "        self.model['b1'] = file['arr_1']\n",
    "        self.model['w2'] = file['arr_2']\n",
    "        self.model['b2'] = file['arr_3']\n",
    "        self.model['w3'] = file['arr_4']\n",
    "        self.model['b3'] = file['arr_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 training Loss: 2.5706731062125354 validation Loss: 2.588169711668722 train accuracy 37.212 valid accuracy 37.15\n",
      "epoch: 1 training Loss: 2.2886838849544304 validation Loss: 2.30852787499621 train accuracy 44.95 valid accuracy 44.82\n",
      "epoch: 2 training Loss: 2.1756990302928974 validation Loss: 2.1957921669255014 train accuracy 48.22 valid accuracy 47.3\n",
      "epoch: 3 training Loss: 2.107914845430586 validation Loss: 2.1279933586392037 train accuracy 50.14999999999999 valid accuracy 49.41\n",
      "epoch: 4 training Loss: 2.0565647683507917 validation Loss: 2.0770765940985028 train accuracy 51.442 valid accuracy 51.07000000000001\n",
      "epoch: 5 training Loss: 2.0135433401611884 validation Loss: 2.034765535264801 train accuracy 52.724000000000004 valid accuracy 52.16\n",
      "epoch: 6 training Loss: 1.9759876827797571 validation Loss: 1.9980130597358348 train accuracy 53.657999999999994 valid accuracy 53.14\n",
      "epoch: 7 training Loss: 1.9426269902237308 validation Loss: 1.9654319681946673 train accuracy 54.604 valid accuracy 53.949999999999996\n",
      "epoch: 8 training Loss: 1.9122157851181258 validation Loss: 1.9359380058813513 train accuracy 55.448 valid accuracy 54.7\n",
      "epoch: 9 training Loss: 1.8867866699015343 validation Loss: 1.9112395057101421 train accuracy 56.13 valid accuracy 55.300000000000004\n",
      "epoch: 10 training Loss: 1.864038070907557 validation Loss: 1.8890676330747789 train accuracy 56.67 valid accuracy 55.779999999999994\n",
      "epoch: 11 training Loss: 1.8430276318563803 validation Loss: 1.8686247320476908 train accuracy 57.218 valid accuracy 56.46\n",
      "epoch: 12 training Loss: 1.8235847510389358 validation Loss: 1.8497410912558738 train accuracy 57.714 valid accuracy 56.95\n",
      "epoch: 13 training Loss: 1.80558305388982 validation Loss: 1.8322945005577138 train accuracy 58.194 valid accuracy 57.38999999999999\n",
      "epoch: 14 training Loss: 1.7888908558825103 validation Loss: 1.816154543444736 train accuracy 58.536 valid accuracy 57.96\n",
      "epoch: 15 training Loss: 1.7733828003825651 validation Loss: 1.801192663327157 train accuracy 58.897999999999996 valid accuracy 58.45\n",
      "epoch: 16 training Loss: 1.7589403376297856 validation Loss: 1.7872847859528196 train accuracy 59.206 valid accuracy 58.74\n",
      "epoch: 17 training Loss: 1.745453912044881 validation Loss: 1.7743149659125221 train accuracy 59.524 valid accuracy 58.97\n",
      "epoch: 18 training Loss: 1.7328264791764354 validation Loss: 1.7621796799607903 train accuracy 59.855999999999995 valid accuracy 59.3\n",
      "epoch: 19 training Loss: 1.7209765817999363 validation Loss: 1.7507915691802787 train accuracy 60.128 valid accuracy 59.61\n",
      "epoch: 20 training Loss: 1.709839517329148 validation Loss: 1.7400809376655009 train accuracy 60.416000000000004 valid accuracy 59.809999999999995\n",
      "epoch: 21 training Loss: 1.699364766418011 validation Loss: 1.7299934850797105 train accuracy 60.692 valid accuracy 60.019999999999996\n",
      "epoch: 22 training Loss: 1.6895093915765054 validation Loss: 1.7204850258582762 train accuracy 60.904 valid accuracy 60.209999999999994\n",
      "epoch: 23 training Loss: 1.6802294182926885 validation Loss: 1.7115158779199526 train accuracy 61.07599999999999 valid accuracy 60.45\n",
      "epoch: 24 training Loss: 1.6714734109401332 validation Loss: 1.7030500447385202 train accuracy 61.332 valid accuracy 60.589999999999996\n",
      "epoch: 25 training Loss: 1.663198042240753 validation Loss: 1.6950724476328156 train accuracy 61.536 valid accuracy 60.79\n",
      "epoch: 26 training Loss: 1.6554255292968283 validation Loss: 1.6875992669086488 train accuracy 61.73800000000001 valid accuracy 60.91\n",
      "epoch: 27 training Loss: 1.6481541265809807 validation Loss: 1.6805842847850705 train accuracy 61.888 valid accuracy 61.040000000000006\n",
      "epoch: 28 training Loss: 1.6413191223718808 validation Loss: 1.6739742289098576 train accuracy 62.024 valid accuracy 61.3\n",
      "epoch: 29 training Loss: 1.6348882818500905 validation Loss: 1.6677503343658329 train accuracy 62.23 valid accuracy 61.53999999999999\n",
      "epoch: 30 training Loss: 1.6288377690704106 validation Loss: 1.6618920002995676 train accuracy 62.326 valid accuracy 61.739999999999995\n",
      "epoch: 31 training Loss: 1.6231415955492694 validation Loss: 1.656375833140388 train accuracy 62.394000000000005 valid accuracy 61.919999999999995\n",
      "epoch: 32 training Loss: 1.6177735921361767 validation Loss: 1.6511786758464413 train accuracy 62.536 valid accuracy 62.06\n",
      "epoch: 33 training Loss: 1.6127089125385403 validation Loss: 1.6462787659758473 train accuracy 62.656 valid accuracy 62.09\n",
      "epoch: 34 training Loss: 1.6079246451156015 validation Loss: 1.6416561686491165 train accuracy 62.766 valid accuracy 62.239999999999995\n",
      "epoch: 35 training Loss: 1.6034000343438666 validation Loss: 1.6372929892816168 train accuracy 62.888 valid accuracy 62.31\n",
      "epoch: 36 training Loss: 1.599116527419008 validation Loss: 1.6331734921644456 train accuracy 62.971999999999994 valid accuracy 62.4\n",
      "epoch: 37 training Loss: 1.59505767676132 validation Loss: 1.6292840768420835 train accuracy 63.046 valid accuracy 62.45\n",
      "epoch: 38 training Loss: 1.5912088368963273 validation Loss: 1.625612977018835 train accuracy 63.126000000000005 valid accuracy 62.55\n",
      "epoch: 39 training Loss: 1.587556610085123 validation Loss: 1.6221495893482523 train accuracy 63.188 valid accuracy 62.55\n",
      "epoch: 40 training Loss: 1.5840881216374394 validation Loss: 1.6188835322692459 train accuracy 63.242 valid accuracy 62.580000000000005\n",
      "epoch: 41 training Loss: 1.5807903497732507 validation Loss: 1.615803751393061 train accuracy 63.31400000000001 valid accuracy 62.62\n",
      "epoch: 42 training Loss: 1.5776497514241492 validation Loss: 1.612898017307623 train accuracy 63.382000000000005 valid accuracy 62.580000000000005\n",
      "epoch: 43 training Loss: 1.5746522782083834 validation Loss: 1.610152950938401 train accuracy 63.449999999999996 valid accuracy 62.629999999999995\n",
      "epoch: 44 training Loss: 1.571783691304914 validation Loss: 1.6075544474045462 train accuracy 63.542 valid accuracy 62.580000000000005\n",
      "epoch: 45 training Loss: 1.56902999678847 validation Loss: 1.6050882516534715 train accuracy 63.598 valid accuracy 62.57\n",
      "epoch: 46 training Loss: 1.5663778500805048 validation Loss: 1.6027404851140477 train accuracy 63.656 valid accuracy 62.660000000000004\n",
      "epoch: 47 training Loss: 1.5638148520246826 validation Loss: 1.6004980287407349 train accuracy 63.712 valid accuracy 62.64999999999999\n",
      "epoch: 48 training Loss: 1.5613297214100588 validation Loss: 1.5983487510307317 train accuracy 63.74999999999999 valid accuracy 62.64999999999999\n",
      "epoch: 49 training Loss: 1.558912361870967 validation Loss: 1.5962816094841321 train accuracy 63.804 valid accuracy 62.67\n",
      "epoch: 50 training Loss: 1.556553851405245 validation Loss: 1.5942866634048833 train accuracy 63.870000000000005 valid accuracy 62.69\n",
      "epoch: 51 training Loss: 1.5542463813144547 validation Loss: 1.592355031428835 train accuracy 63.92400000000001 valid accuracy 62.74999999999999\n",
      "epoch: 52 training Loss: 1.5519831658834273 validation Loss: 1.5904788188252106 train accuracy 63.949999999999996 valid accuracy 62.849999999999994\n",
      "epoch: 53 training Loss: 1.5497583383124616 validation Loss: 1.5886510317364821 train accuracy 63.99399999999999 valid accuracy 62.89\n",
      "epoch: 54 training Loss: 1.547566843548569 validation Loss: 1.5868654892810277 train accuracy 64.046 valid accuracy 63.01\n",
      "epoch: 55 training Loss: 1.545404334880788 validation Loss: 1.5851167398330246 train accuracy 64.106 valid accuracy 63.04\n",
      "epoch: 56 training Loss: 1.543267078290063 validation Loss: 1.5833999845166362 train accuracy 64.152 valid accuracy 63.09\n",
      "epoch: 57 training Loss: 1.5411518663926909 validation Loss: 1.5817110087286042 train accuracy 64.19200000000001 valid accuracy 63.1\n",
      "epoch: 58 training Loss: 1.539055942274465 validation Loss: 1.580046121128292 train accuracy 64.252 valid accuracy 63.190000000000005\n",
      "epoch: 59 training Loss: 1.5369769325134603 validation Loss: 1.578402098842996 train accuracy 64.316 valid accuracy 63.28\n",
      "epoch: 60 training Loss: 1.5349127881706723 validation Loss: 1.5767761374753926 train accuracy 64.386 valid accuracy 63.370000000000005\n",
      "epoch: 61 training Loss: 1.5328617324060398 validation Loss: 1.5751658047103825 train accuracy 64.42 valid accuracy 63.41\n",
      "epoch: 62 training Loss: 1.5308222135434264 validation Loss: 1.57356899673828 train accuracy 64.474 valid accuracy 63.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 63 training Loss: 1.52879286274099 validation Loss: 1.571983897186617 train accuracy 64.51 valid accuracy 63.629999999999995\n",
      "epoch: 64 training Loss: 1.5267724558110498 validation Loss: 1.5704089386507707 train accuracy 64.55199999999999 valid accuracy 63.67\n",
      "epoch: 65 training Loss: 1.5247598790937065 validation Loss: 1.5688427671267364 train accuracy 64.62599999999999 valid accuracy 63.68000000000001\n",
      "epoch: 66 training Loss: 1.52275409957924 validation Loss: 1.5672842095868633 train accuracy 64.644 valid accuracy 63.73\n",
      "epoch: 67 training Loss: 1.5207541396892537 validation Loss: 1.565732244500231 train accuracy 64.672 valid accuracy 63.79\n",
      "epoch: 68 training Loss: 1.518759057263745 validation Loss: 1.5641859741312347 train accuracy 64.712 valid accuracy 63.79\n",
      "epoch: 69 training Loss: 1.5167679312991944 validation Loss: 1.5626445957277593 train accuracy 64.766 valid accuracy 63.75999999999999\n",
      "epoch: 70 training Loss: 1.5147798536299268 validation Loss: 1.5611073660447288 train accuracy 64.832 valid accuracy 63.78\n",
      "epoch: 71 training Loss: 1.512793925651973 validation Loss: 1.5595735504051367 train accuracy 64.862 valid accuracy 63.85999999999999\n",
      "epoch: 72 training Loss: 1.5108092571231255 validation Loss: 1.5580423459795898 train accuracy 64.89 valid accuracy 63.89\n",
      "epoch: 73 training Loss: 1.508824962207828 validation Loss: 1.5565127755970487 train accuracy 64.934 valid accuracy 63.89\n",
      "epoch: 74 training Loss: 1.5068401508506224 validation Loss: 1.5549835736500452 train accuracy 65.008 valid accuracy 63.980000000000004\n",
      "epoch: 75 training Loss: 1.5048539279974091 validation Loss: 1.5534531323555996 train accuracy 65.066 valid accuracy 64.05\n",
      "epoch: 76 training Loss: 1.502865435101202 validation Loss: 1.5519196112781737 train accuracy 65.132 valid accuracy 64.07000000000001\n",
      "epoch: 77 training Loss: 1.5008739652745047 validation Loss: 1.550381258692348 train accuracy 65.184 valid accuracy 64.07000000000001\n",
      "epoch: 78 training Loss: 1.498879121166343 validation Loss: 1.5488368245154303 train accuracy 65.226 valid accuracy 64.08\n",
      "epoch: 79 training Loss: 1.4968809093884425 validation Loss: 1.5472858123552 train accuracy 65.27 valid accuracy 64.16\n",
      "epoch: 80 training Loss: 1.4948796871855108 validation Loss: 1.5457284149120123 train accuracy 65.31 valid accuracy 64.2\n",
      "epoch: 81 training Loss: 1.492875995086287 validation Loss: 1.5441652221848685 train accuracy 65.35600000000001 valid accuracy 64.27000000000001\n",
      "epoch: 82 training Loss: 1.490870385831923 validation Loss: 1.5425969149371865 train accuracy 65.414 valid accuracy 64.34\n",
      "epoch: 83 training Loss: 1.488863327241082 validation Loss: 1.5410240800162545 train accuracy 65.462 valid accuracy 64.35\n",
      "epoch: 84 training Loss: 1.4868551843074462 validation Loss: 1.5394471563575443 train accuracy 65.524 valid accuracy 64.4\n",
      "epoch: 85 training Loss: 1.484846247156289 validation Loss: 1.537866458317925 train accuracy 65.578 valid accuracy 64.42999999999999\n",
      "epoch: 86 training Loss: 1.482836772188572 validation Loss: 1.5362822251635457 train accuracy 65.62599999999999 valid accuracy 64.45\n",
      "epoch: 87 training Loss: 1.480827018720804 validation Loss: 1.534694668864066 train accuracy 65.66199999999999 valid accuracy 64.44\n",
      "epoch: 88 training Loss: 1.4788172757346845 validation Loss: 1.5331040109429968 train accuracy 65.702 valid accuracy 64.45\n",
      "epoch: 89 training Loss: 1.476807879340249 validation Loss: 1.5315105081815479 train accuracy 65.744 valid accuracy 64.53\n",
      "epoch: 90 training Loss: 1.4747992232544727 validation Loss: 1.5299144697341325 train accuracy 65.782 valid accuracy 64.51\n",
      "epoch: 91 training Loss: 1.4727917644165092 validation Loss: 1.5283162682329776 train accuracy 65.83200000000001 valid accuracy 64.53\n",
      "epoch: 92 training Loss: 1.4707860251812972 validation Loss: 1.5267163467010063 train accuracy 65.86999999999999 valid accuracy 64.63\n",
      "epoch: 93 training Loss: 1.4687825929060763 validation Loss: 1.5251152223309952 train accuracy 65.91 valid accuracy 64.73\n",
      "epoch: 94 training Loss: 1.466782117315515 validation Loss: 1.5235134876363696 train accuracy 65.974 valid accuracy 64.79\n",
      "epoch: 95 training Loss: 1.4647853058016722 validation Loss: 1.5219118091484771 train accuracy 66.012 valid accuracy 64.84\n",
      "epoch: 96 training Loss: 1.4627929167480522 validation Loss: 1.5203109236895342 train accuracy 66.042 valid accuracy 64.91\n",
      "epoch: 97 training Loss: 1.460805751019528 validation Loss: 1.518711632245404 train accuracy 66.086 valid accuracy 64.96\n",
      "epoch: 98 training Loss: 1.4588246418860142 validation Loss: 1.5171147915522485 train accuracy 66.142 valid accuracy 65.0\n",
      "epoch: 99 training Loss: 1.4568504437987404 validation Loss: 1.5155213036483262 train accuracy 66.19 valid accuracy 64.96\n",
      "epoch: 100 training Loss: 1.4548840205630553 validation Loss: 1.5139321037780107 train accuracy 66.23400000000001 valid accuracy 65.08\n",
      "epoch: 101 training Loss: 1.4529262335052606 validation Loss: 1.5123481471253029 train accuracy 66.266 valid accuracy 65.18\n",
      "epoch: 102 training Loss: 1.4509779301807733 validation Loss: 1.5107703948679705 train accuracy 66.316 valid accuracy 65.29\n",
      "epoch: 103 training Loss: 1.4490399340087512 validation Loss: 1.5091997999733606 train accuracy 66.33200000000001 valid accuracy 65.44\n",
      "epoch: 104 training Loss: 1.4471130349677968 validation Loss: 1.5076372930250779 train accuracy 66.372 valid accuracy 65.48\n",
      "epoch: 105 training Loss: 1.4451979812079567 validation Loss: 1.5060837682288686 train accuracy 66.396 valid accuracy 65.53\n",
      "epoch: 106 training Loss: 1.4432954712144843 validation Loss: 1.5045400696680917 train accuracy 66.408 valid accuracy 65.57\n",
      "epoch: 107 training Loss: 1.4414061460959517 validation Loss: 1.5030069779307684 train accuracy 66.446 valid accuracy 65.59\n",
      "epoch: 108 training Loss: 1.4395305817310386 validation Loss: 1.5014851974367598 train accuracy 66.50399999999999 valid accuracy 65.64999999999999\n",
      "epoch: 109 training Loss: 1.437669280887282 validation Loss: 1.499975345106485 train accuracy 66.552 valid accuracy 65.64999999999999\n",
      "epoch: 110 training Loss: 1.4358226659107591 validation Loss: 1.4984779413040612 train accuracy 66.584 valid accuracy 65.68\n",
      "epoch: 111 training Loss: 1.433991072985504 validation Loss: 1.4969934040936799 train accuracy 66.63799999999999 valid accuracy 65.79\n",
      "epoch: 112 training Loss: 1.4321747490809713 validation Loss: 1.4955220476512465 train accuracy 66.67 valid accuracy 65.86999999999999\n",
      "epoch: 113 training Loss: 1.4303738524536518 validation Loss: 1.4940640851812037 train accuracy 66.724 valid accuracy 65.95\n",
      "epoch: 114 training Loss: 1.4285884570162366 validation Loss: 1.4926196360469033 train accuracy 66.764 valid accuracy 65.94\n",
      "epoch: 115 training Loss: 1.4268185602352594 validation Loss: 1.4911887362461074 train accuracy 66.808 valid accuracy 65.96\n",
      "epoch: 116 training Loss: 1.425064093688278 validation Loss: 1.4897713510234574 train accuracy 66.818 valid accuracy 66.03999999999999\n",
      "epoch: 117 training Loss: 1.4233249351410038 validation Loss: 1.4883673883630861 train accuracy 66.84400000000001 valid accuracy 66.07\n",
      "epoch: 118 training Loss: 1.421600921003478 validation Loss: 1.4869767122841269 train accuracy 66.904 valid accuracy 66.09\n",
      "epoch: 119 training Loss: 1.419891858209695 validation Loss: 1.4855991551497063 train accuracy 66.934 valid accuracy 66.12\n",
      "epoch: 120 training Loss: 1.418197534827399 validation Loss: 1.4842345284876783 train accuracy 66.97800000000001 valid accuracy 66.18\n",
      "epoch: 121 training Loss: 1.41651772896009 validation Loss: 1.4828826320486823 train accuracy 67.032 valid accuracy 66.24\n",
      "epoch: 122 training Loss: 1.4148522157107037 validation Loss: 1.4815432609827697 train accuracy 67.06800000000001 valid accuracy 66.28\n",
      "epoch: 123 training Loss: 1.4132007721302595 validation Loss: 1.4802162111166504 train accuracy 67.096 valid accuracy 66.31\n",
      "epoch: 124 training Loss: 1.4115631801869062 validation Loss: 1.4789012823836902 train accuracy 67.11200000000001 valid accuracy 66.32000000000001\n",
      "epoch: 125 training Loss: 1.4099392278758829 validation Loss: 1.4775982805161119 train accuracy 67.142 valid accuracy 66.31\n",
      "epoch: 126 training Loss: 1.408328708658915 validation Loss: 1.4763070171615817 train accuracy 67.19000000000001 valid accuracy 66.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 127 training Loss: 1.4067314194756204 validation Loss: 1.4750273086343522 train accuracy 67.23400000000001 valid accuracy 66.36999999999999\n",
      "epoch: 128 training Loss: 1.4051471576077508 validation Loss: 1.4737589735494645 train accuracy 67.284 valid accuracy 66.4\n",
      "epoch: 129 training Loss: 1.4035757166950307 validation Loss: 1.4725018296108496 train accuracy 67.304 valid accuracy 66.44\n",
      "epoch: 130 training Loss: 1.402016882195405 validation Loss: 1.4712556898258735 train accuracy 67.354 valid accuracy 66.47\n",
      "epoch: 131 training Loss: 1.4004704265512882 validation Loss: 1.4700203583979912 train accuracy 67.374 valid accuracy 66.51\n",
      "epoch: 132 training Loss: 1.3989361042693604 validation Loss: 1.4687956265074813 train accuracy 67.41 valid accuracy 66.55\n",
      "epoch: 133 training Loss: 1.3974136470495961 validation Loss: 1.4675812681319724 train accuracy 67.444 valid accuracy 66.57\n",
      "epoch: 134 training Loss: 1.3959027590149045 validation Loss: 1.4663770359885222 train accuracy 67.482 valid accuracy 66.58\n",
      "epoch: 135 training Loss: 1.3944031119991556 validation Loss: 1.465182657600337 train accuracy 67.512 valid accuracy 66.56\n",
      "epoch: 136 training Loss: 1.3929143407455786 validation Loss: 1.4639978314022346 train accuracy 67.584 valid accuracy 66.57\n",
      "epoch: 137 training Loss: 1.3914360377386912 validation Loss: 1.4628222226915855 train accuracy 67.634 valid accuracy 66.57\n",
      "epoch: 138 training Loss: 1.3899677472202925 validation Loss: 1.4616554590903417 train accuracy 67.66 valid accuracy 66.61\n",
      "epoch: 139 training Loss: 1.3885089576925422 validation Loss: 1.4604971249865926 train accuracy 67.706 valid accuracy 66.69\n",
      "epoch: 140 training Loss: 1.3870590918472692 validation Loss: 1.4593467541432827 train accuracy 67.74 valid accuracy 66.72\n",
      "epoch: 141 training Loss: 1.3856174923303413 validation Loss: 1.458203819268949 train accuracy 67.756 valid accuracy 66.72\n",
      "epoch: 142 training Loss: 1.3841834010015295 validation Loss: 1.45706771682219 train accuracy 67.77 valid accuracy 66.72\n",
      "epoch: 143 training Loss: 1.382755928360948 validation Loss: 1.4559377446939223 train accuracy 67.82000000000001 valid accuracy 66.77\n",
      "epoch: 144 training Loss: 1.3813340086870431 validation Loss: 1.4548130698446864 train accuracy 67.864 valid accuracy 66.79\n",
      "epoch: 145 training Loss: 1.3799163356968995 validation Loss: 1.4536926830362114 train accuracy 67.874 valid accuracy 66.86\n",
      "epoch: 146 training Loss: 1.3785012749641798 validation Loss: 1.4525753401247907 train accuracy 67.906 valid accuracy 66.9\n",
      "epoch: 147 training Loss: 1.3770867579354575 validation Loss: 1.4514594982146045 train accuracy 67.938 valid accuracy 66.92\n",
      "epoch: 148 training Loss: 1.3756701906197764 validation Loss: 1.4503432799166136 train accuracy 67.974 valid accuracy 66.92\n",
      "epoch: 149 training Loss: 1.374248482678759 validation Loss: 1.4492245568649558 train accuracy 68.018 valid accuracy 66.96\n",
      "epoch: 150 training Loss: 1.3728184454621073 validation Loss: 1.4481013442334183 train accuracy 68.042 valid accuracy 66.97999999999999\n",
      "epoch: 151 training Loss: 1.3713779543211284 validation Loss: 1.4469727657163969 train accuracy 68.052 valid accuracy 67.03\n",
      "epoch: 152 training Loss: 1.3699280212587148 validation Loss: 1.4458405628248059 train accuracy 68.08800000000001 valid accuracy 67.06\n",
      "epoch: 153 training Loss: 1.3684746367781249 validation Loss: 1.4447101141515155 train accuracy 68.142 valid accuracy 67.10000000000001\n",
      "epoch: 154 training Loss: 1.3670278456566491 validation Loss: 1.44358918755196 train accuracy 68.16600000000001 valid accuracy 67.13\n",
      "epoch: 155 training Loss: 1.365597486669152 validation Loss: 1.4424845711754741 train accuracy 68.19800000000001 valid accuracy 67.19000000000001\n",
      "epoch: 156 training Loss: 1.3641892702756522 validation Loss: 1.4413995831116393 train accuracy 68.23 valid accuracy 67.19000000000001\n",
      "epoch: 157 training Loss: 1.3628042354885235 validation Loss: 1.4403342143377769 train accuracy 68.264 valid accuracy 67.25999999999999\n",
      "epoch: 158 training Loss: 1.3614404578193713 validation Loss: 1.4392866076206192 train accuracy 68.312 valid accuracy 67.30000000000001\n",
      "epoch: 159 training Loss: 1.3600948214270276 validation Loss: 1.4382543076123195 train accuracy 68.352 valid accuracy 67.32000000000001\n",
      "epoch: 160 training Loss: 1.3587640603979512 validation Loss: 1.437234903666729 train accuracy 68.384 valid accuracy 67.34\n",
      "epoch: 161 training Loss: 1.3574452017384262 validation Loss: 1.43622626379052 train accuracy 68.42200000000001 valid accuracy 67.36\n",
      "epoch: 162 training Loss: 1.356135683306989 validation Loss: 1.4352265712542662 train accuracy 68.446 valid accuracy 67.38\n",
      "epoch: 163 training Loss: 1.3548333251441609 validation Loss: 1.4342342801566743 train accuracy 68.476 valid accuracy 67.35\n",
      "epoch: 164 training Loss: 1.3535362530248056 validation Loss: 1.4332480503024396 train accuracy 68.514 valid accuracy 67.36\n",
      "epoch: 165 training Loss: 1.3522428391048025 validation Loss: 1.432266704378493 train accuracy 68.54400000000001 valid accuracy 67.35\n",
      "epoch: 166 training Loss: 1.3509517154516997 validation Loss: 1.43128924955138 train accuracy 68.58 valid accuracy 67.47999999999999\n",
      "epoch: 167 training Loss: 1.3496619043032883 validation Loss: 1.4303149984594188 train accuracy 68.61 valid accuracy 67.44\n",
      "epoch: 168 training Loss: 1.3483730608970224 validation Loss: 1.4293437835347518 train accuracy 68.636 valid accuracy 67.47\n",
      "epoch: 169 training Loss: 1.3470857207920035 validation Loss: 1.428376167909839 train accuracy 68.678 valid accuracy 67.49000000000001\n",
      "epoch: 170 training Loss: 1.345801337400371 validation Loss: 1.4274134657478128 train accuracy 68.71000000000001 valid accuracy 67.49000000000001\n",
      "epoch: 171 training Loss: 1.3445219378882602 validation Loss: 1.4264574286064278 train accuracy 68.738 valid accuracy 67.54\n",
      "epoch: 172 training Loss: 1.3432494908173638 validation Loss: 1.425509691600217 train accuracy 68.77 valid accuracy 67.53\n",
      "epoch: 173 training Loss: 1.3419853384998606 validation Loss: 1.4245712942476327 train accuracy 68.816 valid accuracy 67.58\n",
      "epoch: 174 training Loss: 1.3407299910881172 validation Loss: 1.423642528296545 train accuracy 68.858 valid accuracy 67.65\n",
      "epoch: 175 training Loss: 1.3394832808560089 validation Loss: 1.422723097591005 train accuracy 68.87 valid accuracy 67.67\n",
      "epoch: 176 training Loss: 1.3382446711641862 validation Loss: 1.421812402409125 train accuracy 68.894 valid accuracy 67.64\n",
      "epoch: 177 training Loss: 1.3370135324780885 validation Loss: 1.4209097845609961 train accuracy 68.946 valid accuracy 67.67\n",
      "epoch: 178 training Loss: 1.3357893114104378 validation Loss: 1.4200146726353544 train accuracy 68.982 valid accuracy 67.67\n",
      "epoch: 179 training Loss: 1.3345716017258777 validation Loss: 1.419126639103255 train accuracy 69.02000000000001 valid accuracy 67.7\n",
      "epoch: 180 training Loss: 1.333360153408263 validation Loss: 1.4182454031968903 train accuracy 69.054 valid accuracy 67.75\n",
      "epoch: 181 training Loss: 1.3321548518896118 validation Loss: 1.417370808644662 train accuracy 69.096 valid accuracy 67.77\n",
      "epoch: 182 training Loss: 1.3309556875605066 validation Loss: 1.4165027940795625 train accuracy 69.134 valid accuracy 67.78\n",
      "epoch: 183 training Loss: 1.3297627257187032 validation Loss: 1.4156413648686808 train accuracy 69.16600000000001 valid accuracy 67.78\n",
      "epoch: 184 training Loss: 1.3285760810262326 validation Loss: 1.4147865696803648 train accuracy 69.174 valid accuracy 67.80000000000001\n",
      "epoch: 185 training Loss: 1.327395897431916 validation Loss: 1.4139384823890062 train accuracy 69.202 valid accuracy 67.81\n",
      "epoch: 186 training Loss: 1.3262223331618508 validation Loss: 1.4130971887789867 train accuracy 69.236 valid accuracy 67.82000000000001\n",
      "epoch: 187 training Loss: 1.325055549925019 validation Loss: 1.4122627771621046 train accuracy 69.27 valid accuracy 67.83\n",
      "epoch: 188 training Loss: 1.323895705431668 validation Loss: 1.4114353320223676 train accuracy 69.31 valid accuracy 67.83\n",
      "epoch: 189 training Loss: 1.322742948428329 validation Loss: 1.4106149299270883 train accuracy 69.356 valid accuracy 67.85\n",
      "epoch: 190 training Loss: 1.3215974156003134 validation Loss: 1.4098016370943414 train accuracy 69.382 valid accuracy 67.83\n",
      "epoch: 191 training Loss: 1.3204592298334412 validation Loss: 1.4089955081454175 train accuracy 69.39999999999999 valid accuracy 67.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 192 training Loss: 1.319328499446073 validation Loss: 1.408196585685536 train accuracy 69.462 valid accuracy 67.85\n",
      "epoch: 193 training Loss: 1.3182053180982916 validation Loss: 1.4074049004466285 train accuracy 69.494 valid accuracy 67.86999999999999\n",
      "epoch: 194 training Loss: 1.3170897651598392 validation Loss: 1.4066204717959347 train accuracy 69.52199999999999 valid accuracy 67.95\n",
      "epoch: 195 training Loss: 1.3159819063760902 validation Loss: 1.4058433084676885 train accuracy 69.554 valid accuracy 67.95\n",
      "epoch: 196 training Loss: 1.3148817947154532 validation Loss: 1.405073409415943 train accuracy 69.6 valid accuracy 67.99\n",
      "epoch: 197 training Loss: 1.3137894713152842 validation Loss: 1.4043107647176085 train accuracy 69.634 valid accuracy 67.97\n",
      "epoch: 198 training Loss: 1.3127049664688466 validation Loss: 1.4035553564783245 train accuracy 69.66 valid accuracy 67.97999999999999\n",
      "epoch: 199 training Loss: 1.311628300614986 validation Loss: 1.4028071597115195 train accuracy 69.66 valid accuracy 68.02\n",
      "epoch: 200 training Loss: 1.3105594853063425 validation Loss: 1.4020661431742272 train accuracy 69.674 valid accuracy 68.05\n",
      "epoch: 201 training Loss: 1.30949852414204 validation Loss: 1.4013322701527933 train accuracy 69.71199999999999 valid accuracy 68.07\n",
      "epoch: 202 training Loss: 1.3084454136577652 validation Loss: 1.4006054991981982 train accuracy 69.732 valid accuracy 68.08\n",
      "epoch: 203 training Loss: 1.3074001441705054 validation Loss: 1.3998857848148003 train accuracy 69.754 valid accuracy 68.12\n",
      "epoch: 204 training Loss: 1.306362700577487 validation Loss: 1.3991730781082863 train accuracy 69.796 valid accuracy 68.16\n",
      "epoch: 205 training Loss: 1.3053330631095033 validation Loss: 1.3984673273987864 train accuracy 69.82199999999999 valid accuracy 68.19\n",
      "epoch: 206 training Loss: 1.3043112080382229 validation Loss: 1.397768478803842 train accuracy 69.854 valid accuracy 68.17\n",
      "epoch: 207 training Loss: 1.303297108335786 validation Loss: 1.397076476793624 train accuracy 69.864 valid accuracy 68.16\n",
      "epoch: 208 training Loss: 1.302290734283269 validation Loss: 1.3963912647178451 train accuracy 69.884 valid accuracy 68.17999999999999\n",
      "epoch: 209 training Loss: 1.3012920540231638 validation Loss: 1.395712785300823 train accuracy 69.89800000000001 valid accuracy 68.2\n",
      "epoch: 210 training Loss: 1.3003010340499477 validation Loss: 1.3950409810985003 train accuracy 69.934 valid accuracy 68.23\n",
      "epoch: 211 training Loss: 1.299317639632662 validation Loss: 1.394375794909444 train accuracy 69.946 valid accuracy 68.2\n",
      "epoch: 212 training Loss: 1.298341835164244 validation Loss: 1.3937171701312472 train accuracy 69.974 valid accuracy 68.23\n",
      "epoch: 213 training Loss: 1.2973735844341188 validation Loss: 1.393065051054455 train accuracy 69.978 valid accuracy 68.25\n",
      "epoch: 214 training Loss: 1.296412850823225 validation Loss: 1.3924193830881408 train accuracy 70.00800000000001 valid accuracy 68.26\n",
      "epoch: 215 training Loss: 1.2954595974236935 validation Loss: 1.391780112914216 train accuracy 70.02000000000001 valid accuracy 68.25\n",
      "epoch: 216 training Loss: 1.2945137870887329 validation Loss: 1.3911471885712043 train accuracy 70.04 valid accuracy 68.28999999999999\n",
      "epoch: 217 training Loss: 1.293575382421153 validation Loss: 1.3905205594718852 train accuracy 70.052 valid accuracy 68.32000000000001\n",
      "epoch: 218 training Loss: 1.2926443457113357 validation Loss: 1.389900176362609 train accuracy 70.07600000000001 valid accuracy 68.34\n",
      "epoch: 219 training Loss: 1.291720638836842 validation Loss: 1.3892859912346394 train accuracy 70.11800000000001 valid accuracy 68.36\n",
      "epoch: 220 training Loss: 1.290804223136379 validation Loss: 1.3886779571995471 train accuracy 70.15599999999999 valid accuracy 68.36\n",
      "epoch: 221 training Loss: 1.2898950592703384 validation Loss: 1.388076028341173 train accuracy 70.166 valid accuracy 68.41000000000001\n",
      "epoch: 222 training Loss: 1.2889931070789324 validation Loss: 1.3874801595562138 train accuracy 70.204 valid accuracy 68.42\n",
      "epoch: 223 training Loss: 1.2880983254471756 validation Loss: 1.3868903063942395 train accuracy 70.218 valid accuracy 68.42\n",
      "epoch: 224 training Loss: 1.2872106721839238 validation Loss: 1.3863064249060728 train accuracy 70.236 valid accuracy 68.38\n",
      "epoch: 225 training Loss: 1.2863301039201318 validation Loss: 1.3857284715074172 train accuracy 70.26 valid accuracy 68.42\n",
      "epoch: 226 training Loss: 1.2854565760296315 validation Loss: 1.3851564028624832 train accuracy 70.28 valid accuracy 68.43\n",
      "epoch: 227 training Loss: 1.2845900425741024 validation Loss: 1.3845901757904313 train accuracy 70.308 valid accuracy 68.43\n",
      "epoch: 228 training Loss: 1.2837304562728666 validation Loss: 1.384029747195896 train accuracy 70.336 valid accuracy 68.47999999999999\n",
      "epoch: 229 training Loss: 1.2828777684973434 validation Loss: 1.3834750740236545 train accuracy 70.342 valid accuracy 68.47999999999999\n",
      "epoch: 230 training Loss: 1.2820319292898115 validation Loss: 1.3829261132367923 train accuracy 70.352 valid accuracy 68.45\n",
      "epoch: 231 training Loss: 1.2811928874062075 validation Loss: 1.3823828218174021 train accuracy 70.37 valid accuracy 68.43\n",
      "epoch: 232 training Loss: 1.2803605903832147 validation Loss: 1.3818451567889587 train accuracy 70.378 valid accuracy 68.45\n",
      "epoch: 233 training Loss: 1.2795349846306303 validation Loss: 1.3813130752599019 train accuracy 70.382 valid accuracy 68.5\n",
      "epoch: 234 training Loss: 1.278716015550988 validation Loss: 1.3807865344886445 train accuracy 70.418 valid accuracy 68.52000000000001\n",
      "epoch: 235 training Loss: 1.2779036276895277 validation Loss: 1.3802654919710544 train accuracy 70.43599999999999 valid accuracy 68.55\n",
      "epoch: 236 training Loss: 1.2770977649187794 validation Loss: 1.3797499055524334 train accuracy 70.452 valid accuracy 68.55\n",
      "epoch: 237 training Loss: 1.276298370663146 validation Loss: 1.3792397335669366 train accuracy 70.476 valid accuracy 68.56\n",
      "epoch: 238 training Loss: 1.2755053881698097 validation Loss: 1.378734935008229 train accuracy 70.50800000000001 valid accuracy 68.57\n",
      "epoch: 239 training Loss: 1.274718760832861 validation Loss: 1.3782354697357926 train accuracy 70.518 valid accuracy 68.58999999999999\n",
      "epoch: 240 training Loss: 1.2739384325775074 validation Loss: 1.377741298721416 train accuracy 70.552 valid accuracy 68.64\n",
      "epoch: 241 training Loss: 1.2731643483102286 validation Loss: 1.3772523843399747 train accuracy 70.56400000000001 valid accuracy 68.65\n",
      "epoch: 242 training Loss: 1.272396454438446 validation Loss: 1.3767686907071528 train accuracy 70.57600000000001 valid accuracy 68.65\n",
      "epoch: 243 training Loss: 1.2716346994591692 validation Loss: 1.3762901840641246 train accuracy 70.596 valid accuracy 68.67999999999999\n",
      "epoch: 244 training Loss: 1.2708790346096586 validation Loss: 1.3758168332049372 train accuracy 70.616 valid accuracy 68.67999999999999\n",
      "epoch: 245 training Loss: 1.2701294145640445 validation Loss: 1.3753486099361394 train accuracy 70.636 valid accuracy 68.72\n",
      "epoch: 246 training Loss: 1.2693857981477776 validation Loss: 1.3748854895498708 train accuracy 70.65 valid accuracy 68.7\n",
      "epoch: 247 training Loss: 1.2686481490269765 validation Loss: 1.3744274512811563 train accuracy 70.678 valid accuracy 68.71000000000001\n",
      "epoch: 248 training Loss: 1.2679164363131066 validation Loss: 1.3739744787080765 train accuracy 70.706 valid accuracy 68.72\n",
      "epoch: 249 training Loss: 1.2671906350068012 validation Loss: 1.373526560040765 train accuracy 70.722 valid accuracy 68.67999999999999\n",
      "epoch: 250 training Loss: 1.2664707261910242 validation Loss: 1.37308368823389 train accuracy 70.75200000000001 valid accuracy 68.7\n",
      "epoch: 251 training Loss: 1.2657566968773868 validation Loss: 1.3726458608499816 train accuracy 70.768 valid accuracy 68.7\n",
      "epoch: 252 training Loss: 1.2650485394150017 validation Loss: 1.372213079601259 train accuracy 70.77799999999999 valid accuracy 68.71000000000001\n",
      "epoch: 253 training Loss: 1.2643462503932992 validation Loss: 1.3717853495090329 train accuracy 70.784 valid accuracy 68.73\n",
      "epoch: 254 training Loss: 1.263649829011311 validation Loss: 1.3713626776451373 train accuracy 70.78 valid accuracy 68.75\n",
      "epoch: 255 training Loss: 1.2629592749450136 validation Loss: 1.3709450714598768 train accuracy 70.798 valid accuracy 68.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 256 training Loss: 1.262274585815309 validation Loss: 1.3705325367531933 train accuracy 70.804 valid accuracy 68.74\n",
      "epoch: 257 training Loss: 1.261595754430759 validation Loss: 1.3701250754033845 train accuracy 70.822 valid accuracy 68.74\n",
      "epoch: 258 training Loss: 1.260922766036453 validation Loss: 1.3697226830206792 train accuracy 70.842 valid accuracy 68.78999999999999\n",
      "epoch: 259 training Loss: 1.2602555958283865 validation Loss: 1.3693253467291273 train accuracy 70.872 valid accuracy 68.77\n",
      "epoch: 260 training Loss: 1.2595942069813382 validation Loss: 1.3689330432892461 train accuracy 70.88 valid accuracy 68.78999999999999\n",
      "epoch: 261 training Loss: 1.2589385493857146 validation Loss: 1.3685457377498242 train accuracy 70.894 valid accuracy 68.81\n",
      "epoch: 262 training Loss: 1.258288559203597 validation Loss: 1.3681633827621 train accuracy 70.922 valid accuracy 68.82000000000001\n",
      "epoch: 263 training Loss: 1.2576441592530312 validation Loss: 1.3677859186124561 train accuracy 70.938 valid accuracy 68.82000000000001\n",
      "epoch: 264 training Loss: 1.257005260132421 validation Loss: 1.367413273945502 train accuracy 70.928 valid accuracy 68.8\n",
      "epoch: 265 training Loss: 1.2563717619216643 validation Loss: 1.3670453670738651 train accuracy 70.952 valid accuracy 68.81\n",
      "epoch: 266 training Loss: 1.2557435562543071 validation Loss: 1.3666821077170388 train accuracy 70.974 valid accuracy 68.8\n",
      "epoch: 267 training Loss: 1.2551205285477574 validation Loss: 1.3663233989859696 train accuracy 71.004 valid accuracy 68.8\n",
      "epoch: 268 training Loss: 1.2545025602010298 validation Loss: 1.3659691394326523 train accuracy 71.044 valid accuracy 68.82000000000001\n",
      "epoch: 269 training Loss: 1.2538895306115623 validation Loss: 1.3656192250091408 train accuracy 71.07 valid accuracy 68.81\n",
      "epoch: 270 training Loss: 1.2532813189129315 validation Loss: 1.3652735508194402 train accuracy 71.088 valid accuracy 68.84\n",
      "epoch: 271 training Loss: 1.252677805384354 validation Loss: 1.3649320125916113 train accuracy 71.11 valid accuracy 68.84\n",
      "epoch: 272 training Loss: 1.2520788725239107 validation Loss: 1.364594507838625 train accuracy 71.134 valid accuracy 68.88\n",
      "epoch: 273 training Loss: 1.2514844058075605 validation Loss: 1.3642609367102874 train accuracy 71.14200000000001 valid accuracy 68.87\n",
      "epoch: 274 training Loss: 1.250894294174749 validation Loss: 1.3639312025626853 train accuracy 71.148 valid accuracy 68.87\n",
      "epoch: 275 training Loss: 1.250308430290203 validation Loss: 1.3636052122860727 train accuracy 71.166 valid accuracy 68.87\n",
      "epoch: 276 training Loss: 1.2497267106328342 validation Loss: 1.3632828764383973 train accuracy 71.186 valid accuracy 68.91000000000001\n",
      "epoch: 277 training Loss: 1.2491490354588992 validation Loss: 1.3629641092316531 train accuracy 71.186 valid accuracy 68.93\n",
      "epoch: 278 training Loss: 1.2485753086800169 validation Loss: 1.3626488284141682 train accuracy 71.218 valid accuracy 68.94\n",
      "epoch: 279 training Loss: 1.2480054376889202 validation Loss: 1.3623369550855506 train accuracy 71.22800000000001 valid accuracy 68.89999999999999\n",
      "epoch: 280 training Loss: 1.2474393331582312 validation Loss: 1.3620284134738423 train accuracy 71.25399999999999 valid accuracy 68.91000000000001\n",
      "epoch: 281 training Loss: 1.246876908830599 validation Loss: 1.3617231306973416 train accuracy 71.276 valid accuracy 68.89999999999999\n",
      "epoch: 282 training Loss: 1.2463180813127874 validation Loss: 1.3614210365272357 train accuracy 71.292 valid accuracy 68.93\n",
      "epoch: 283 training Loss: 1.2457627698816818 validation Loss: 1.3611220631618801 train accuracy 71.306 valid accuracy 68.94\n",
      "epoch: 284 training Loss: 1.2452108963066637 validation Loss: 1.3608261450193189 train accuracy 71.308 valid accuracy 68.89999999999999\n",
      "epoch: 285 training Loss: 1.2446623846902933 validation Loss: 1.3605332185514352 train accuracy 71.31 valid accuracy 68.89999999999999\n",
      "epoch: 286 training Loss: 1.2441171613275026 validation Loss: 1.36024322208083 train accuracy 71.312 valid accuracy 68.89999999999999\n",
      "epoch: 287 training Loss: 1.2435751545824207 validation Loss: 1.3599560956599501 train accuracy 71.328 valid accuracy 68.89999999999999\n",
      "epoch: 288 training Loss: 1.2430362947812512 validation Loss: 1.359671780950984 train accuracy 71.334 valid accuracy 68.88\n",
      "epoch: 289 training Loss: 1.2425005141193541 validation Loss: 1.35939022112448 train accuracy 71.33800000000001 valid accuracy 68.88\n",
      "epoch: 290 training Loss: 1.2419677465805927 validation Loss: 1.3591113607744227 train accuracy 71.358 valid accuracy 68.89\n",
      "epoch: 291 training Loss: 1.2414379278670171 validation Loss: 1.3588351458473995 train accuracy 71.372 valid accuracy 68.89\n",
      "epoch: 292 training Loss: 1.2409109953371598 validation Loss: 1.358561523583652 train accuracy 71.37400000000001 valid accuracy 68.89999999999999\n",
      "epoch: 293 training Loss: 1.2403868879513535 validation Loss: 1.3582904424679398 train accuracy 71.39 valid accuracy 68.91000000000001\n",
      "epoch: 294 training Loss: 1.2398655462227233 validation Loss: 1.3580218521883916 train accuracy 71.396 valid accuracy 68.94\n",
      "epoch: 295 training Loss: 1.2393469121726843 validation Loss: 1.3577557036017316 train accuracy 71.394 valid accuracy 68.96\n",
      "epoch: 296 training Loss: 1.2388309292899664 validation Loss: 1.3574919487035098 train accuracy 71.41199999999999 valid accuracy 69.03\n",
      "epoch: 297 training Loss: 1.2383175424923911 validation Loss: 1.3572305406021863 train accuracy 71.41 valid accuracy 69.08999999999999\n",
      "epoch: 298 training Loss: 1.2378066980907192 validation Loss: 1.356971433496096 train accuracy 71.422 valid accuracy 69.1\n",
      "epoch: 299 training Loss: 1.2372983437540683 validation Loss: 1.3567145826525049 train accuracy 71.426 valid accuracy 69.07\n",
      "epoch: 300 training Loss: 1.2367924284764853 validation Loss: 1.3564599443881205 train accuracy 71.42399999999999 valid accuracy 69.07\n",
      "epoch: 301 training Loss: 1.2362889025443486 validation Loss: 1.356207476050527 train accuracy 71.41999999999999 valid accuracy 69.1\n",
      "epoch: 302 training Loss: 1.2357877175043561 validation Loss: 1.3559571360001488 train accuracy 71.43 valid accuracy 69.12\n",
      "epoch: 303 training Loss: 1.2352888261319024 validation Loss: 1.3557088835923958 train accuracy 71.456 valid accuracy 69.15\n",
      "epoch: 304 training Loss: 1.2347921823997112 validation Loss: 1.3554626791597573 train accuracy 71.462 valid accuracy 69.19\n",
      "epoch: 305 training Loss: 1.2342977414466196 validation Loss: 1.3552184839936374 train accuracy 71.466 valid accuracy 69.19\n",
      "epoch: 306 training Loss: 1.2338054595464363 validation Loss: 1.3549762603257827 train accuracy 71.48400000000001 valid accuracy 69.19\n",
      "epoch: 307 training Loss: 1.2333152940768126 validation Loss: 1.3547359713091855 train accuracy 71.504 valid accuracy 69.17999999999999\n",
      "epoch: 308 training Loss: 1.2328272034881045 validation Loss: 1.354497580998389 train accuracy 71.5 valid accuracy 69.17999999999999\n",
      "epoch: 309 training Loss: 1.2323411472721852 validation Loss: 1.3542610543291163 train accuracy 71.49799999999999 valid accuracy 69.19999999999999\n",
      "epoch: 310 training Loss: 1.2318570859312086 validation Loss: 1.354026357097186 train accuracy 71.5 valid accuracy 69.21000000000001\n",
      "epoch: 311 training Loss: 1.2313749809462835 validation Loss: 1.3537934559366784 train accuracy 71.52199999999999 valid accuracy 69.23\n",
      "epoch: 312 training Loss: 1.2308947947460893 validation Loss: 1.353562318297332 train accuracy 71.538 valid accuracy 69.24\n",
      "epoch: 313 training Loss: 1.2304164906753885 validation Loss: 1.353332912421143 train accuracy 71.556 valid accuracy 69.22\n",
      "epoch: 314 training Loss: 1.2299400329634613 validation Loss: 1.3531052073181702 train accuracy 71.566 valid accuracy 69.22\n",
      "epoch: 315 training Loss: 1.2294653866924332 validation Loss: 1.3528791727415188 train accuracy 71.58200000000001 valid accuracy 69.19999999999999\n",
      "epoch: 316 training Loss: 1.2289925177655112 validation Loss: 1.352654779161525 train accuracy 71.598 valid accuracy 69.23\n",
      "epoch: 317 training Loss: 1.2285213928751026 validation Loss: 1.3524319977390957 train accuracy 71.61 valid accuracy 69.26\n",
      "epoch: 318 training Loss: 1.2280519794708222 validation Loss: 1.3522108002982511 train accuracy 71.622 valid accuracy 69.28\n",
      "epoch: 319 training Loss: 1.2275842457273753 validation Loss: 1.3519911592978282 train accuracy 71.626 valid accuracy 69.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 320 training Loss: 1.2271181605122987 validation Loss: 1.3517730478023642 train accuracy 71.63199999999999 valid accuracy 69.28\n",
      "epoch: 321 training Loss: 1.2266536933535737 validation Loss: 1.3515564394521649 train accuracy 71.638 valid accuracy 69.35\n",
      "epoch: 322 training Loss: 1.22619081440707 validation Loss: 1.351341308432554 train accuracy 71.64399999999999 valid accuracy 69.37\n",
      "epoch: 323 training Loss: 1.2257294944238344 validation Loss: 1.3511276294423151 train accuracy 71.652 valid accuracy 69.39999999999999\n",
      "epoch: 324 training Loss: 1.225269704717211 validation Loss: 1.3509153776613356 train accuracy 71.67 valid accuracy 69.39\n",
      "epoch: 325 training Loss: 1.2248114171297704 validation Loss: 1.3507045287174588 train accuracy 71.686 valid accuracy 69.39999999999999\n",
      "epoch: 326 training Loss: 1.2243546040000666 validation Loss: 1.3504950586525815 train accuracy 71.694 valid accuracy 69.39\n",
      "epoch: 327 training Loss: 1.2238992381291967 validation Loss: 1.3502869438879979 train accuracy 71.708 valid accuracy 69.39999999999999\n",
      "epoch: 328 training Loss: 1.2234452927471766 validation Loss: 1.350080161189027 train accuracy 71.706 valid accuracy 69.39\n",
      "epoch: 329 training Loss: 1.2229927414791288 validation Loss: 1.3498746876289698 train accuracy 71.712 valid accuracy 69.39\n",
      "epoch: 330 training Loss: 1.222541558311294 validation Loss: 1.349670500552423 train accuracy 71.714 valid accuracy 69.38\n",
      "epoch: 331 training Loss: 1.2220917175568786 validation Loss: 1.3494675775380134 train accuracy 71.72 valid accuracy 69.39\n",
      "epoch: 332 training Loss: 1.2216431938217616 validation Loss: 1.3492658963606114 train accuracy 71.744 valid accuracy 69.38\n",
      "epoch: 333 training Loss: 1.221195961970083 validation Loss: 1.3490654349531037 train accuracy 71.76599999999999 valid accuracy 69.38\n",
      "epoch: 334 training Loss: 1.2207499970897535 validation Loss: 1.3488661713678018 train accuracy 71.78 valid accuracy 69.41000000000001\n",
      "epoch: 335 training Loss: 1.2203052744579295 validation Loss: 1.348668083737598 train accuracy 71.792 valid accuracy 69.43\n",
      "epoch: 336 training Loss: 1.2198617695065141 validation Loss: 1.348471150236975 train accuracy 71.798 valid accuracy 69.46\n",
      "epoch: 337 training Loss: 1.2194194577877357 validation Loss: 1.348275349043004 train accuracy 71.814 valid accuracy 69.47\n",
      "epoch: 338 training Loss: 1.2189783149399067 validation Loss: 1.3480806582964804 train accuracy 71.82600000000001 valid accuracy 69.49\n",
      "epoch: 339 training Loss: 1.21853831665344 validation Loss: 1.3478870560633638 train accuracy 71.832 valid accuracy 69.5\n",
      "epoch: 340 training Loss: 1.2180994386372272 validation Loss: 1.3476945202966926 train accuracy 71.848 valid accuracy 69.52000000000001\n",
      "epoch: 341 training Loss: 1.217661656585531 validation Loss: 1.347503028799208 train accuracy 71.87 valid accuracy 69.51\n",
      "epoch: 342 training Loss: 1.2172249461454878 validation Loss: 1.3473125591868722 train accuracy 71.892 valid accuracy 69.52000000000001\n",
      "epoch: 343 training Loss: 1.216789282885416 validation Loss: 1.3471230888535572 train accuracy 71.884 valid accuracy 69.52000000000001\n",
      "epoch: 344 training Loss: 1.2163546422640839 validation Loss: 1.3469345949371434 train accuracy 71.898 valid accuracy 69.52000000000001\n",
      "epoch: 345 training Loss: 1.2159209996011295 validation Loss: 1.3467470542873243 train accuracy 71.91 valid accuracy 69.54\n",
      "epoch: 346 training Loss: 1.2154883300488444 validation Loss: 1.346560443435409 train accuracy 71.92399999999999 valid accuracy 69.56\n",
      "epoch: 347 training Loss: 1.2150566085655545 validation Loss: 1.346374738566441 train accuracy 71.946 valid accuracy 69.6\n",
      "epoch: 348 training Loss: 1.2146258098908076 validation Loss: 1.346189915493954 train accuracy 71.95 valid accuracy 69.63000000000001\n",
      "epoch: 349 training Loss: 1.2141959085226628 validation Loss: 1.346005949637724 train accuracy 71.964 valid accuracy 69.61\n",
      "epoch: 350 training Loss: 1.213766878697315 validation Loss: 1.3458228160048447 train accuracy 71.964 valid accuracy 69.6\n",
      "epoch: 351 training Loss: 1.2133386943713467 validation Loss: 1.3456404891744886 train accuracy 71.976 valid accuracy 69.59\n",
      "epoch: 352 training Loss: 1.2129113292068783 validation Loss: 1.345458943286709 train accuracy 72.002 valid accuracy 69.62\n",
      "epoch: 353 training Loss: 1.2124847565599168 validation Loss: 1.3452781520356167 train accuracy 72.02199999999999 valid accuracy 69.61\n",
      "epoch: 354 training Loss: 1.2120589494721707 validation Loss: 1.3450980886672799 train accuracy 72.026 valid accuracy 69.64\n",
      "epoch: 355 training Loss: 1.2116338806666191 validation Loss: 1.3449187259826478 train accuracy 72.032 valid accuracy 69.64\n",
      "epoch: 356 training Loss: 1.2112095225471013 validation Loss: 1.344740036345801 train accuracy 72.04 valid accuracy 69.64\n",
      "epoch: 357 training Loss: 1.210785847202168 validation Loss: 1.344561991697778 train accuracy 72.042 valid accuracy 69.66\n",
      "epoch: 358 training Loss: 1.2103628264134403 validation Loss: 1.344384563576207 train accuracy 72.052 valid accuracy 69.69999999999999\n",
      "epoch: 359 training Loss: 1.209940431668652 validation Loss: 1.3442077231409062 train accuracy 72.062 valid accuracy 69.69\n",
      "epoch: 360 training Loss: 1.2095186341795623 validation Loss: 1.3440314412055772 train accuracy 72.078 valid accuracy 69.69\n",
      "epoch: 361 training Loss: 1.2090974049048393 validation Loss: 1.3438556882756423 train accuracy 72.088 valid accuracy 69.67999999999999\n",
      "epoch: 362 training Loss: 1.2086767145780088 validation Loss: 1.3436804345922289 train accuracy 72.092 valid accuracy 69.72\n",
      "epoch: 363 training Loss: 1.2082565337404747 validation Loss: 1.3435056501821956 train accuracy 72.11 valid accuracy 69.73\n",
      "epoch: 364 training Loss: 1.207836832779575 validation Loss: 1.3433313049140607 train accuracy 72.11999999999999 valid accuracy 69.74000000000001\n",
      "epoch: 365 training Loss: 1.2074175819715822 validation Loss: 1.3431573685595648 train accuracy 72.124 valid accuracy 69.73\n",
      "epoch: 366 training Loss: 1.206998751529465 validation Loss: 1.3429838108605527 train accuracy 72.142 valid accuracy 69.72\n",
      "epoch: 367 training Loss: 1.206580311655182 validation Loss: 1.3428106016007506 train accuracy 72.14 valid accuracy 69.74000000000001\n",
      "epoch: 368 training Loss: 1.2061622325962036 validation Loss: 1.3426377106819374 train accuracy 72.154 valid accuracy 69.73\n",
      "epoch: 369 training Loss: 1.2057444847058607 validation Loss: 1.342465108203917 train accuracy 72.166 valid accuracy 69.75\n",
      "epoch: 370 training Loss: 1.205327038507099 validation Loss: 1.3422927645476337 train accuracy 72.184 valid accuracy 69.73\n",
      "epoch: 371 training Loss: 1.2049098647591037 validation Loss: 1.3421206504606793 train accuracy 72.172 valid accuracy 69.78\n",
      "epoch: 372 training Loss: 1.2044929345262387 validation Loss: 1.341948737144405 train accuracy 72.178 valid accuracy 69.78\n",
      "epoch: 373 training Loss: 1.2040762192486403 validation Loss: 1.341776996341736 train accuracy 72.188 valid accuracy 69.77\n",
      "epoch: 374 training Loss: 1.203659690813839 validation Loss: 1.3416054004248394 train accuracy 72.182 valid accuracy 69.78\n",
      "epoch: 375 training Loss: 1.2032433216286582 validation Loss: 1.3414339224816514 train accuracy 72.18599999999999 valid accuracy 69.78999999999999\n",
      "epoch: 376 training Loss: 1.2028270846906688 validation Loss: 1.3412625364003397 train accuracy 72.188 valid accuracy 69.77\n",
      "epoch: 377 training Loss: 1.2024109536584822 validation Loss: 1.3410912169507434 train accuracy 72.198 valid accuracy 69.78999999999999\n",
      "epoch: 378 training Loss: 1.201994902920093 validation Loss: 1.340919939861838 train accuracy 72.20400000000001 valid accuracy 69.8\n",
      "epoch: 379 training Loss: 1.2015789076585854 validation Loss: 1.340748681894323 train accuracy 72.21600000000001 valid accuracy 69.81\n",
      "epoch: 380 training Loss: 1.2011629439144877 validation Loss: 1.340577420907487 train accuracy 72.218 valid accuracy 69.8\n",
      "epoch: 381 training Loss: 1.200746988644132 validation Loss: 1.3404061359195396 train accuracy 72.24199999999999 valid accuracy 69.83\n",
      "epoch: 382 training Loss: 1.2003310197734192 validation Loss: 1.3402348071607233 train accuracy 72.246 valid accuracy 69.83\n",
      "epoch: 383 training Loss: 1.1999150162464733 validation Loss: 1.3400634161185911 train accuracy 72.248 valid accuracy 69.85\n",
      "epoch: 384 training Loss: 1.1994989580687432 validation Loss: 1.3398919455749587 train accuracy 72.25200000000001 valid accuracy 69.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 385 training Loss: 1.1990828263442017 validation Loss: 1.3397203796341641 train accuracy 72.248 valid accuracy 69.87\n",
      "epoch: 386 training Loss: 1.1986666033063813 validation Loss: 1.3395487037423957 train accuracy 72.25399999999999 valid accuracy 69.87\n",
      "epoch: 387 training Loss: 1.1982502723431239 validation Loss: 1.3393769046980057 train accuracy 72.262 valid accuracy 69.87\n",
      "epoch: 388 training Loss: 1.1978338180149692 validation Loss: 1.3392049706528484 train accuracy 72.28 valid accuracy 69.89\n",
      "epoch: 389 training Loss: 1.197417226067259 validation Loss: 1.3390328911048277 train accuracy 72.292 valid accuracy 69.89999999999999\n",
      "epoch: 390 training Loss: 1.1970004834361225 validation Loss: 1.3388606568819992 train accuracy 72.306 valid accuracy 69.89999999999999\n",
      "epoch: 391 training Loss: 1.1965835782485759 validation Loss: 1.3386882601186736 train accuracy 72.316 valid accuracy 69.93\n",
      "epoch: 392 training Loss: 1.196166499817098 validation Loss: 1.338515694224093 train accuracy 72.32 valid accuracy 69.89999999999999\n",
      "epoch: 393 training Loss: 1.1957492386290698 validation Loss: 1.3383429538443747 train accuracy 72.324 valid accuracy 69.89\n",
      "epoch: 394 training Loss: 1.1953317863315696 validation Loss: 1.3381700348184744 train accuracy 72.33000000000001 valid accuracy 69.89999999999999\n",
      "epoch: 395 training Loss: 1.194914135712003 validation Loss: 1.337996934129003 train accuracy 72.344 valid accuracy 69.92\n",
      "epoch: 396 training Loss: 1.1944962806751553 validation Loss: 1.3378236498487917 train accuracy 72.358 valid accuracy 69.94\n",
      "epoch: 397 training Loss: 1.1940782162171697 validation Loss: 1.337650181084074 train accuracy 72.36 valid accuracy 69.95\n",
      "epoch: 398 training Loss: 1.1936599383970348 validation Loss: 1.337476527915218 train accuracy 72.36 valid accuracy 69.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 399 training Loss: 1.1932414443060955 validation Loss: 1.3373026913358674 train accuracy 72.36200000000001 valid accuracy 69.98\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn = Neural_Network([128,64,10])\n",
    "nn.train(trainX, trainY, batch_size = 128, epochs = 400, \\\n",
    "         learningRate = 0.1, validationX = validX, validationY = validY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.saveModel(\"BestModelCNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.loadModel('BestModelCNN.npz')\n",
    "nn.accuracy(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 769,    3,   37,   29,   20,   17,   26,   14,   64,    1],\n",
       "       [   1, 1104,    6,    3,    2,    2,    2,    2,   11,    2],\n",
       "       [  27,   11,  741,   80,   47,   11,   15,   31,   65,    4],\n",
       "       [  11,   15,  101,  608,   43,   32,   18,   23,  149,   10],\n",
       "       [  16,    8,   35,   29,  661,    7,   38,   11,  107,   70],\n",
       "       [  15,    6,   42,  134,   38,  522,   18,   11,   97,    9],\n",
       "       [  41,    7,   24,   39,   66,   43,  651,    3,   79,    5],\n",
       "       [  11,   12,   89,   40,   47,   12,    2,  719,   65,   31],\n",
       "       [  22,    4,   31,   38,   36,   23,   17,    7,  783,   13],\n",
       "       [  12,    7,   47,   65,  198,   17,   14,   28,  129,  492]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = nn.predict(testX)\n",
    "p = np.argmax(y_pred, axis=1)\n",
    "y = np.argmax(test_set_y, axis=1)\n",
    "confusion_matrix(y, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
